---
layout: default
title: World-Model-Driven Reinforcement Learning AI Agents for Stable Fulfillment Routing
---

<!-- MathJax (required for LaTeX equations on GitHub Pages) -->
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  };
</script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

> World-model-driven AI agents for stable and cost-efficient fulfillment routing under real, bursty demand.

---

## 1. Problem Statement

Modern fulfillment networks operate under volatile, bursty demand, finite processing capacity, and strict service-level requirements. Orders arrive continuously across multiple customer regions, while fulfillment centers can process only a limited number of orders per time interval. When routing decisions fail to account for system stress and backlog dynamics, instability can propagate over time, leading to persistent delays and disproportionate cost escalation.

Traditional routing heuristics and purely reactive reinforcement learning approaches optimize decisions based only on the current system state. They do not explicitly model how queues evolve, how stress accumulates, or how present actions shape future system behavior. In capacity-constrained fulfillment networks subject to demand shocks, this lack of foresight results in unstable routing, delayed recovery, and degraded service performance.

This project formulates fulfillment routing as a sequential decision-making and planning problem and studies whether AI agents augmented with learned world models can achieve stable and resilient system behavior. The agent learns not only a routing policy, but also an internal predictive model of fulfillment dynamics, enabling it to anticipate system stress, simulate future outcomes, and plan routing decisions proactively.

We investigate two complementary approaches:

World-Model-Driven Planning, where an AI agent uses a learned dynamics model to perform model-predictive control and select routing actions that minimize long-term cost.

Hybrid Model-Based Reinforcement Learning, where imagined trajectories generated by the world model are used to improve policy learning efficiency and robustness.

Using real, bursty demand derived from large-scale urban data, we evaluate whether world-model-driven AI agents can outperform static heuristics and model-free reinforcement learning in terms of cost, stability, and resilience to demand shocks.

---

## 2. Dataset and Demand Modeling

### 2.1 Data Source

We use New York City Taxi and Limousine Commission trip data as a proxy for customer order arrivals. Each taxi pickup is treated as an order arrival originating from a geographic zone at a specific time.

This dataset is well suited for modeling fulfillment demand because it exhibits:
- Large scale with millions of events
- Strong daily and weekly cycles
- Sudden bursts that stress capacity-constrained systems

Only pickup timestamps and pickup zones are used. No transportation behavior from the taxi system is modeled.

---

### 2.2 Time Aggregation and Zone Compression

Raw trip-level events are aggregated into fixed five-minute time buckets. For each bucket, demand is counted per pickup zone.

To control dimensionality while preserving realism:
- The top 30 highest-demand zones are retained as individual demand sources
- All remaining zones are aggregated into a single composite group labeled **OTHER**

This preserves demand concentration effects while keeping the routing problem tractable.

---

### 2.3 Exploratory Data Analysis

#### Demand Distribution per Zone and Time Bucket

![](assets/demand_hist.png)

Demand is highly right-skewed. Most zoneâ€“time pairs have low demand, while a small fraction experience extreme spikes. This heavy-tailed behavior is a primary driver of congestion.

---

#### Top Zones by Total Demand

![](assets/top_zones.png)

A small subset of zones dominates total demand volume. This justifies focusing routing decisions on a reduced set of high-impact zones.

---

#### Total Demand Over Time

![](assets/total_demand_timeseries.png)

The time series shows strong periodic structure with sharp peaks. These peaks are the stress events where naive routing policies fail.

---

## 3. Fulfillment Network Simulator

We model a simplified fulfillment network with multiple fulfillment centers, each represented as a queue with finite processing capacity.

At each discrete time step:
1. New demand arrives from each zone
2. A routing policy assigns demand to fulfillment centers
3. Each center processes orders up to its capacity
4. Unprocessed orders remain in queue as backlog

Backlog represents delayed or late orders and captures congestion effects.

---

### 3.1 System Dynamics

Let:
- $q_i(t)$ be the queue length at fulfillment center $i$ at time $t$
- $a_i(t)$ be arrivals routed to center $i$
- $c_i$ be the processing capacity of center $i$

Queue evolution follows:

$$
q_i(t+1) = \max \left( q_i(t) + a_i(t) - c_i,\ 0 \right)
$$

This introduces temporal coupling, meaning poor routing decisions can harm system performance over many future steps.

---

## 4. Baseline Routing Policies

Before introducing reinforcement learning, we evaluate three baseline routing strategies to establish reference behavior, identify failure modes, and quantify the cost of ignoring congestion dynamics.

These baselines serve as control policies against which learning-based approaches will later be compared.

---

### 4.1 Myopic Cheapest Routing

Each demand zone routes all incoming orders to the fulfillment center with the lowest per-order shipping cost.

$$
\pi(z) = \arg\min_i \; \text{cost}_{z,i}
$$

Where:
- $z$ denotes a demand zone
- $i$ indexes fulfillment centers
- $\text{cost}_{z,i}$ represents the marginal shipping or processing cost

This policy is locally optimal at each decision step but ignores system state, queue length, and future consequences.

**Observed behavior:**
- Rapid overload of the cheapest fulfillment centers
- Large and persistent queues during demand surges
- Extreme cost spikes driven by late-order penalties
- Poor recovery after congestion events due to backlog accumulation

The myopic policy consistently exhibits catastrophic failures under peak demand, making it unsuitable for realistic operations.

#### ðŸ“Š Figures
- Queue trajectories under myopic routing  
  ![](assets/queues_myopic_cheapest.png)

  **How to read this:** queues rise sharply in specific fulfillment centers and remain elevated during peak demand windows. This indicates the policy repeatedly routes into already congested centers.

- Step cost over time under myopic routing  
  ![](assets/cost_myopic_cheapest.png)

  **How to read this:** cost spikes are concentrated around high-demand intervals. These spikes reflect compounding penalties from backlog and late orders, not just marginal shipping cost.

- Late backlog over time under myopic routing  
  ![](assets/late_myopic_cheapest.png)

  **How to read this:** backlog grows quickly and clears slowly. This is congestion persistence, where poor routing decisions create delayed effects that carry forward.

---

### 4.2 Static Capacity-Proportional Split

Incoming demand is split across fulfillment centers in proportion to their processing capacities, independent of real-time congestion.

$$
a_i(t) = D(t) \cdot \frac{c_i}{\sum_j c_j}
$$

Where:
- $D(t)$ is total system demand at time $t$
- $c_i$ is the processing capacity of fulfillment center $i$

This policy enforces long-run load balance but does not adapt to short-term fluctuations.

**Observed behavior:**
- More evenly distributed queues across fulfillment centers
- Significantly reduced cost volatility compared to myopic routing
- Lower average backlog during normal demand conditions
- Inability to adapt during extreme demand spikes or sudden regime shifts

Static splitting trades responsiveness for stability and performs well only when demand patterns remain predictable.

#### ðŸ“Š Figures
- Queue trajectories under static split routing  
  ![](assets/queues_static_split.png)

  **How to read this:** queues remain comparatively bounded and balanced across centers. This indicates load is distributed in a stable manner rather than concentrating in one place.

- Step cost over time under static split routing  
  ![](assets/cost_static_split.png)

  **How to read this:** cost is smoother with fewer extreme spikes. Residual peaks usually occur when total demand exceeds the combined ability of the system to process within the bucket cadence.

- Late backlog over time under static split routing  
  ![](assets/late_static_split.png)

  **How to read this:** late backlog still appears during spikes, but it is smaller and clears faster than under myopic routing.

---

### 4.3 Threshold-Based Congestion Routing

This heuristic augments cheapest-first routing by avoiding fulfillment centers whose congestion exceeds a predefined threshold.

$$
\text{Avoid } i \text{ if } \frac{q_i(t)}{c_i} > \rho_{\text{threshold}}
$$

Where:
- $q_i(t)$ is the queue length at fulfillment center $i$
- $c_i$ is its processing capacity
- $\rho_{\text{threshold}}$ is a congestion tolerance parameter

Demand is routed to the next cheapest feasible fulfillment center when thresholds are violated.

**Observed behavior:**
- Substantial reduction in extreme backlog compared to myopic routing
- Lower cost spikes during peak demand
- Improved recovery after congestion events
- Sensitivity to threshold tuning
- Oscillatory queue behavior caused by hard switching decisions

While threshold-based routing introduces feedback, it remains a brittle rule-based approach that requires careful calibration.

#### ðŸ“Š Figures
- Queue trajectories under threshold-based routing  
  ![](assets/queues_threshold_congestion.png)

  **How to read this:** queues are constrained relative to myopic routing, but you may see switching effects where load shifts quickly between centers as thresholds are crossed.

- Step cost over time under threshold-based routing  
  ![](assets/cost_threshold_congestion.png)

  **How to read this:** cost spikes are reduced versus myopic routing, but not eliminated. The remaining peaks show the limits of simple threshold logic under bursty demand.

- Late backlog over time under threshold-based routing  
  ![](assets/late_threshold_congestion.png)

  **How to read this:** backlog peaks are smaller and recovery is faster, but performance depends heavily on the chosen threshold parameter.

---


## 5. Key Insights from Baseline Analysis

The baseline experiments reveal several important insights:

- Congestion has memory. Once queues build up, recovery takes time even after demand drops
- Greedy, short-term optimal policies can be globally unstable
- Static heuristics trade efficiency for stability
- Hand-crafted congestion rules are brittle and hard to tune

These results motivate reinforcement learning as a principled approach to learning routing policies that optimize **long-term system behavior** rather than immediate cost.

---

## 6. Motivation for Reinforcement Learning

The fulfillment routing problem naturally fits a reinforcement learning framework:

- **State:** queue levels, capacity utilization, recent demand
- **Action:** routing decisions across fulfillment centers
- **Reward:** negative cost incorporating shipping, backlog, and congestion penalties

Reinforcement learning allows the agent to anticipate demand spikes, balance queues proactively, and minimize long-run operational cost.

---

## 7. Reinforcement Learning Formulation

This section defines the exact Markov Decision Process (MDP) used by the agent, including the state, action space, cost function, and the corresponding reward signal used for learning.

### 7.1 State (Observation)

At each time step $t$, the agent receives a continuous observation vector:

$$
s_t = [q_1(t), q_2(t), q_3(t), u_1(t), u_2(t), u_3(t), D(t), \bar{D}(t)]
$$

Where:
- $q_i(t)$ is the backlog (queue length) at fulfillment center $i$ after processing at time $t$
- $u_i(t)$ is a utilization proxy defined as $u_i(t) = \frac{q_i(t)}{\max(c_i, 1)}$
- $D(t)$ is total system demand arriving in time bucket $t$
- $\bar{D}(t)$ is a rolling mean of recent demand over a fixed window (for short-term demand context)

This state captures both instantaneous congestion and short-term demand pressure.

### 7.2 Action Space

The current environment uses a discrete action space:

$$
a_t \in \{0, 1, 2\}
$$

Mapped to three routing rules:
- $a_t = 0$: route all demand using cheapest-first routing
- $a_t = 1$: split demand proportionally to fulfillment center capacities
- $a_t = 2$: threshold-based stress-aware routing (avoid stressed centers, then choose cheapest)

This discrete structure is intentional. It enables stable deep RL baselines (DQN variants) and creates an interpretable control layer. The agent is learning when each routing regime is appropriate as demand conditions change.

### 7.3 Per-Step Cost Function (What the simulator optimizes)

At time step $t$, routing produces a vector of routed demand:

$$
r_t = [r_1(t), r_2(t), r_3(t)]
$$

Where $r_i(t) \ge 0$ and $\sum_i r_i(t) = D(t)$.

The simulator computes total per-step cost as:

$$
C_t = C^{\text{ship}}_t + C^{\text{late}}_t + C^{\text{overflow}}_t + C^{\text{balance}}_t
$$

Each term corresponds directly to operational realities in fulfillment networks.

#### (1) Shipping / Processing Cost

$$
C^{\text{ship}}_t = \sum_i r_i(t)\cdot s_i
$$

Where:
- $s_i$ is the per-order cost associated with fulfillment center $i$ (shipping or processing cost)

This encourages routing toward cheaper centers, but only when the system can handle the load.

#### (2) Late Backlog Penalty (Congestion Persistence)

Let backlog after processing be $q_i(t)$ and define total backlog:

$$
L(t) = \sum_i q_i(t)
$$

Then:

$$
C^{\text{late}}_t = \lambda_{\text{late}} \cdot L(t)
$$

Where:
- $\lambda_{\text{late}}$ is the per-order late penalty

This term is what creates long-horizon behavior. If the agent triggers backlog, it continues paying until the system recovers.

#### (3) Overflow Penalty (Severe Overload)

Define a stress ratio:

$$
\rho_i(t) = \frac{q_i(t)}{\max(c_i, \epsilon)}
$$

Overflow activates only when stress exceeds a hard limit (a â€œmeltdownâ€ regime). In the simulator, overflow begins when $\rho_i(t) > 2$:

$$
C^{\text{overflow}}_t = \lambda_{\text{overflow}}\cdot \sum_i \max(\rho_i(t) - 2,\ 0)
$$

Where:
- $\lambda_{\text{overflow}}$ controls how harshly extreme overload is punished
- $\epsilon$ is a small constant to avoid division by zero

This term prevents the learned policy from sacrificing stability to chase cheap shipping.

#### (4) Balance / Stability Penalty (Queue Imbalance)

To discourage routing that causes one center to accumulate disproportionate backlog, the simulator adds a variance penalty:

$$
C^{\text{balance}}_t = \lambda_{\text{var}} \cdot \text{Var}\left(q(t)\right)
$$

Where:
- $q(t) = [q_1(t), q_2(t), q_3(t)]$
- $\lambda_{\text{var}}$ scales the penalty

This is a stability regularizer. It encourages smoother load distribution without enforcing a rigid split.

### 7.4 Reward Function (What the RL agent learns from)

The RL reward is defined as negative cost:

$$
R_t = -C_t
$$

The training objective is to maximize expected discounted return:

$$
J(\pi) = \mathbb{E}_\pi\left[\sum_{t=0}^{T-1}\gamma^t R_t\right]
= -\mathbb{E}_\pi\left[\sum_{t=0}^{T-1}\gamma^t C_t\right]
$$

Where:
- $\pi$ is the policy
- $\gamma \in (0,1]$ is the discount factor
- $T$ is the episode length (number of time buckets)

Interpretation:
- maximizing $J(\pi)$ means minimizing long-run operational cost
- the agent is trained to make routing choices that reduce future congestion, not just immediate shipping cost

---

## 8. Reinforcement Learning Algorithms to be Implemented

This project targets both model-free deep reinforcement learning and world-model-driven decision-making. The implementations focus on stable, interpretable baselines first, then expand into model-based planning using learned dynamics.

### 8.1 Model-Free Deep RL (DQN Family)

The environment has:
- a discrete action space
- continuous state features
- dense per-step rewards

This is a strong fit for DQN-style algorithms.

The following algorithms are to be implemented:

#### Deep Q-Network (DQN)
Learn an action-value function $Q_\theta(s,a)$ with a neural network and act via:

$$
a_t = \arg\max_a Q_\theta(s_t, a)
$$

Training uses:
- experience replay
- target network stabilization
- $\epsilon$-greedy exploration

#### Double DQN
Reduces Q-value overestimation by decoupling action selection and evaluation in the target:

$$
y_t = R_t + \gamma Q_{\theta^-}\left(s_{t+1}, \arg\max_a Q_\theta(s_{t+1}, a)\right)
$$

Where:
- $\theta$ are online network parameters
- $\theta^-$ are target network parameters

#### Dueling DQN
Decomposes $Q(s,a)$ into state value and action advantage:

$$
Q(s,a) = V(s) + \left(A(s,a) - \frac{1}{|\mathcal{A}|}\sum_{a'}A(s,a')\right)
$$

This is useful when state quality matters strongly, and actions are few and structured (as in this routing setup).

#### Prioritized Experience Replay (optional enhancement)
Samples transitions with higher TD-error more often to improve learning efficiency in rare-but-critical demand spikes.

### 8.2 World-Model-Driven Planning (Model Predictive Control)

A learned world model $\hat{f}$ approximates environment dynamics:

$$
\hat{s}_{t+1},\ \hat{C}_t = \hat{f}_\phi(s_t, a_t)
$$

Planning uses the model to simulate candidate action sequences over a horizon $H$ and chooses the action that minimizes predicted cost:

$$
a_t^* = \arg\min_{a_t,\dots,a_{t+H-1}}
\sum_{k=0}^{H-1}\gamma^k \hat{C}_{t+k}
$$

This is model-predictive control (MPC) in a learned simulator:
- it adds foresight under bursty demand
- it explicitly plans to avoid future congestion, not only react to it

### 8.3 Hybrid Model-Based Reinforcement Learning (Dyna-Style)

The learned world model can generate imagined rollouts:

$$
(s_t, a_t, \hat{R}_t, \hat{s}_{t+1})
$$

These synthetic transitions are mixed into the replay buffer alongside real experience. This approach:
- improves sample efficiency
- amplifies rare demand spike situations without requiring repeated real rollouts
- increases robustness to demand shocks

---

## 9. World Model (Learned Dynamics)

The project includes a learned predictive model trained from rollout data to approximate short-horizon fulfillment dynamics and cost.

The world model learns a mapping:

$$
(s_t, a_t) \rightarrow (\hat{C}_t,\ \hat{q}(t+1))
$$

Where:
- $\hat{C}_t$ is predicted step cost
- $\hat{q}(t+1)$ is predicted next backlog state (queues)

This model is used in two ways:
- as a forward simulator for planning (MPC)
- as a generator of imagined experience (hybrid model-based RL)

---

## 10. Future Work

Next steps include:
- Wrapping the simulator as a Gym-style RL environment
- Training DQN-based agents for congestion-aware routing
- Evaluating performance against baseline heuristics
- Incorporating geographic structure using real zone coordinates and distance-based costs

Geography is intentionally introduced later to keep the core control problem interpretable.

---

## 11. Summary

This project demonstrates that fulfillment routing under real, bursty demand is a dynamic control problem with delayed consequences. Baseline heuristics expose clear failure modes and motivate reinforcement learning as a powerful tool for congestion-aware decision making in supply chain systems.
